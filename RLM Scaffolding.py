import os
import sys
import io
import re
import contextlib
import traceback
import textwrap
import concurrent.futures
import google.generativeai as genai
from typing import List, Tuple, Optional

# --- CONFIGURATION ---
# In a real deployment, these would be env vars.
# We default sub-models to Flash for speed/cost, and Root to Pro (or Flash) for reasoning.
ROOT_MODEL_NAME = "gemini-2.0-flash-exp" 
SUB_MODEL_NAME = "gemini-2.0-flash-exp"

# Placeholder for API Key (In production, use os.environ)
API_KEY = "" # Set this or rely on environment variable

class RLMEnv:
    """
    The Recursive Language Model Environment.
    Manages the 'context' variable and the Python REPL.
    """
    def __init__(self, context_str: str, api_key: str = None):
        self.context = context_str
        self.api_key = api_key or os.environ.get("GOOGLE_API_KEY")
        
        if not self.api_key:
            raise ValueError("Google API Key is required. Set GOOGLE_API_KEY env var or pass to init.")
            
        genai.configure(api_key=self.api_key)
        
        # Persistent local scope for the REPL
        self.local_scope = {
            "context": self.context,
            "llm_query": self.llm_query,
            "llm_batch": self.llm_batch,
            "re": re,
            "print": print # In case we need to override later
        }

    def llm_query(self, prompt: str, content: str) -> str:
        """
        The recursive function exposed to the Root Model.
        Sends a chunk of content to a fresh 'Sub-Model' instance.
        """
        try:
            model = genai.GenerativeModel(SUB_MODEL_NAME)
            # We wrap the content to ensure clear separation
            full_prompt = f"""
            You are a sub-processing unit for a Recursive Language Model.
            
            TASK: {prompt}
            
            DATA CHUNK:
            ---
            {content}
            ---
            
            Return ONLY the requested information. If not found, be concise.
            """
            response = model.generate_content(full_prompt)
            return response.text.strip()
        except Exception as e:
            return f"Error in sub-model query: {str(e)}"

    def llm_batch(self, queries: List[Tuple[str, str]]) -> List[str]:
        """
        Parallel processing for llm_query.
        Input: List of (prompt, content) tuples.
        Output: List of string responses in order.
        """
        results = [None] * len(queries)
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_to_idx = {
                executor.submit(self.llm_query, prompt, content): i 
                for i, (prompt, content) in enumerate(queries)
            }
            
            for future in concurrent.futures.as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    data = future.result()
                    results[idx] = data
                except Exception as exc:
                    results[idx] = f"Error: {exc}"
        return results

    def execute_python_block(self, code: str) -> str:
        """
        Executes Python code generated by the Root Model within the env.
        Captures stdout and returns it as a string.
        """
        # Safety Warning: exec() is used here for demonstration.
        # In production, use a sandboxed E2B environment or Docker container.
        buffer = io.StringIO()
        
        try:
            with contextlib.redirect_stdout(buffer):
                exec(code, {}, self.local_scope)
            output = buffer.getvalue()
            return output if output else "[Code executed successfully. No output.]"
        except Exception:
            return f"Traceback:\n{traceback.format_exc()}"

    def run_root_loop(self, user_query: str, max_turns: int = 5):
        """
        The main loop for the Root Model.
        1. Sends system prompt + user query.
        2. Parses Python code blocks from response.
        3. Executes code.
        4. Feeds output back to Root Model.
        """
        
        # Load the System Prompt (from the file we generated)
        try:
            with open("rlm_instructions.md", "r") as f:
                system_instruction = f.read()
        except FileNotFoundError:
            # Fallback if file isn't found immediately
            system_instruction = "You are a Recursive Language Model. Use Python to inspect the 'context' variable."

        model = genai.GenerativeModel(
            ROOT_MODEL_NAME, 
            system_instruction=system_instruction
        )
        
        chat = model.start_chat(history=[])
        
        # Initial Message
        current_prompt = f"USER QUERY: {user_query}\n\nREMINDER: The data is in the variable `context`. You must write Python code to read it."
        
        print(f"--- Starting RLM Loop (Max Turns: {max_turns}) ---")
        print(f"User Query: {user_query}\n")

        for turn in range(max_turns):
            print(f"--- Turn {turn + 1} ---")
            
            # 1. Call Root Model
            try:
                response = chat.send_message(current_prompt)
                model_text = response.text
                print(f"[Root Model]:\n{textwrap.indent(model_text, '  ')}\n")
            except Exception as e:
                print(f"API Error: {e}")
                break

            # 2. Extract Code Blocks
            # Regex to find ```python ... ``` or just ``` ...